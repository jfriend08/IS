\documentclass[final]{siamltexmm}
\documentclass[10pt,a4paper]{article}

\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{mathtools}
\usepackage{amsmath}

% \usepackage[demo]{graphicx}
% \usepackage{subfig}

\newcommand{\pe}{\psi}
\def\d{\delta} 
\def\ds{\displaystyle} 
\def\e{{\epsilon}} 
\def\eb{\bar{\eta}}  
\def\enorm#1{\|#1\|_2} 
\def\Fp{F^\prime}  
\def\fishpack{{FISHPACK}} 
\def\fortran{{FORTRAN}} 
\def\gmres{{GMRES}} 
\def\gmresm{{\rm GMRES($m$)}} 
\def\Kc{{\cal K}} 
\def\norm#1{\|#1\|} 
\def\wb{{\bar w}} 
\def\zb{{\bar z}} 

% some definitions of bold math italics to make typing easier.
% They are used in the corollary.

\def\bfE{\mbox{\boldmath$E$}}
\def\bfG{\mbox{\boldmath$G$}}

\title{Independent Study -- Learning Music Structure by Laplacian Formula}
\author{Peter Yun-shao Sung\thanks{\tt yss265@nyu.edu} }

\begin{document}
\maketitle

\begin{abstract}
There are many approaches to analyzing music structure by features extracted from dimension of time series. With contruction of similarity matrix, repeated pattern can be captured which is the building block for large-scale structure. This is the work based on the Laplacian Matrix, which is essential start point of spectral clustering. We introduce variables that are trainable to reduce the cost of Laplacian Matrix from true lable, and run this method on wide variable of music recordings. Finally, we demonstrate using these trained variable for performing proper music segmentation.
\end{abstract}

\pagestyle{myheadings}
\thispagestyle{plain}

\section{Laplacian formula}
Normalized Laplacian matrix is the essential start point for identify music segmentation, and the correct boundary detection is done in my baseline approach (Ref2). For proper boundary detection, we woule like to train the initial laplacian matrix ($L$) close to true laplacian ($L^{\ast}$) from true inerval annotation from SALAMI dataset. Therefore, to train and update the model, this section is for deriving the $L$ and ${\partial L \over \partial w_{i,j}}$\\
Given the definition of normalized laplacian matrix:
\begin{equation}
L := I - D^{1 \over 2}W D^{1 \over 2}
\end{equation}
D is degree matrix defined as the diagnal matrix with degrees $d_1, d_2, \ldots, d_n$, which $d_i$ is accumulation of similarity coefficient $w_{i,j}$ betoween time point $i$ and $j$ which defined as followed:
\begin{equation}
d_i = \displaystyle\sum_{j \neq i}^{n} w_{ij}
\end{equation}

After multiplication and the result of equation 1.1 can be rewrite as:
\begin{equation}
L := I - D^{1 \over 2}W D^{1 \over 2} =
\begin{pmatrix}
  1 & {-w_{12} \over \sqrt{d_1d_2}} & \cdots & {-w_{1n} \over \sqrt{d_1d_n}} \\
  {-w_{21} \over \sqrt{d_2d_1}} & 1 & \cdots & {-w_{2n} \over \sqrt{d_2d_n}} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  {-w_{n1} \over \sqrt{d_nd_1}} & {-w_{n2} \over \sqrt{d_nd_2}} & \cdots & 1 \\
\end{pmatrix}
=
\begin{cases}
  {-w_{i,j} \over \sqrt{d_id_j}}       & \quad \text{if } i \neq j\\
  1   & \quad \text{if } i = j\\
\end{cases}
\end{equation}

To take the derivative of $L$ w.r.t $w_{i,j}$. Results is as follow and detail derivation is in appendix 2.1:
\begin{equation}
{\partial L \over \partial w_{i,j}} =
\begin{cases}
  0       & \quad \text{, if $i = j$}\\
  {-1 \over \sqrt{d_id_j}} + {w_{i,j}(d_i+d_j) \over 2(d_id_i)^{3\over 2}} & \quad \text{, for position $(i,j), (j,i)$} \\
  {w_{l,k} \over 2\sqrt{d_k}d_l^{3\over2}}       & \quad \text{, for all position $(l,k), (k,l)$, where $k \neq i \& j$ and $l = i\|j$}\\
  0 & \quad \text{for any other position} \\
\end{cases}
\end{equation}
The key idea of this derivation is that, when taking derivative of $w_{i,j}$ all elements on the i-th and j-th row and column will be altered because of the degree changes in equation 1.2.

\section{Models}
We like to build the model minimize the loss $J$ between $L$ and $L^{\ast}$, which defined as followed:
\begin{equation}
J := {1 \over 2} || L^{\ast} - L ||^2_2
=
{1 \over 2}
\sum_{\substack{i}}
\sum_{\substack{j}}
(L^{\ast}_{i,j} - L_{i,j})^2
\end{equation}

With the loss function defined, we would like to design our model with trainable variables $\theta$ that minimizing the loss fuction during the update:
\begin{equation}
\theta^{t+1} = \theta^{t} - \alpha {\partial J \over \partial \theta}
\end{equation}

\subsection{Model 1-- train on sigma}
Here we design our model with trainable varialbe $\sigma_{i,j}$, which is gaussian width to define the similarity coefficient $w_{i,j}$ between features $x_i$ and $x_j$:
\begin{equation}
w_{i,j} = exp(- ({|| x_i - x_j ||_2 \over \sigma_{i,j} })^2)
\end{equation}
As we like to minimize the loss during each of the $\sigma_{i,j}$ update (eqation 2.2), the derivitive need to expand by chain rule:
\begin{equation}
\begin{aligned}
{\partial J \over \partial \sigma_{i,j}} &= sum[(L - L^{\ast}) \odot {\partial L \over \partial w_{i,j}}] \cdot {\partial w_{i,j} \over \partial \sigma_{i,j}}
\end{aligned}
\end{equation}
Where $\odot$ and $sum$ are element-wise multiplication and summation respectively. The idea of this derivation is when there is slight changes on $\sigma_{i,j}$, it directly affcts $w_{i,j}$ and also all elements on i-th and j-th row and column by the degree in equation 1.2. Figure 2.1 is the test result and the relative error between numerical and analytical method are all below 1e-5.

\begin{figure}[H]
  \centering
    \includegraphics[width=0.8\textwidth]{../fig/Ana_vs_num_relativeErr.png}
  \caption{Deriative of loss w.r.t $\sigma_{i,j}$. Relative error $|f_a - f_n| \over max(|f_a|, |f_n|)$ per each try}
\end{figure}

Combining equation 2.4 and 2.2, we performed the update for each of $\sigma_{i,j}$. We are training the variables for minimizing loss on laplacian, but since laplacian is normalized version of recurrent matrix that values are hard to visualize, therefore we all showing the result on recurrence matrix. Figure 2.2 is the recurrent matrix after sigma being trained. With boundary detection (Ref2) performed, the detected boundaries are identical. The video of recurrence matrix updates over each steps can is also available in Ref3.

\begin{figure}[H]
\centering
\begin{subfigure}
  \begin{tabular}{c}
  \includegraphics[width=50mm]{../fig/UpdateTest_num_singleII_Alpha100000_err.png}
  \end{tabular}{}
\end{subfigure}
  \begin{tabular}{c}
  \includegraphics[width=80mm]{../fig/boundary_sigma.png}
  \end{tabular}{}
\begin{subfigure}
\end{subfigure}
\caption{Left: loss per step. Recurrence matrix of Middle: true sigment annotation, and Right: feature with trained sigma }
\end{figure}

\subsection{Model 2-- train on Q}

\begin{thebibliography}{10}
\bibitem{fpf} {\sc A Tutorial on Spectral Clustering}
\bibitem{fpf} \hyperref[baseline]{''https://github.com/jfriend08/IS/blob/master/reports/midway/report.pdf''}
\bibitem{fpf} \hyperref[baseline]{''https://www.youtube.com/watch?v=XmyKnymXB2Y''}
\end{thebibliography}

\section{Appendix}
\subsection{Differential of Laplacian Matrix}
If we would like to take derivative of Laplacian w.r.t variable $w_{i,j}$ in the symmetric matrix $W$. Basically, except for $L_{i,j}$ the components of $L_{i,k}$, $L_{k,i}$, $L_{k,j}$, $L_{j,k}$ will need to consider.\\
For position $L_{i,j}$:
\begin{equation}
\begin{aligned}
L_{i,j} &= L_{j,i} =  {-w_{i,j} \over \sqrt{d_id_j}} \\
{\partial L_{i,j} \over \partial w_{i,j}} &= {\partial L_{j,i} \over \partial w_{i,j}} = {-1 \over \sqrt{d_id_j}} + {w_{i,j} \over 2(d_id_i)^{3\over 2}}({\partial d_i \over \partial w_{i,j}}d_j + d_i{\partial d_j \over \partial w_{i,j}}) \\
&= {-1 \over \sqrt{d_id_j}} + {w_{i,j}(d_i+d_j) \over 2(d_id_i)^{3\over 2}}
\end{aligned}
\end{equation}
For position $L_{l,k}$, $L_{k,l}$, where $k \neq i \& j$ and $l = i\|j$:
\begin{equation}
\begin{aligned}
L_{k,l} &= L_{l,k} =  {-w_{k,l} \over \sqrt{d_kd_l}} \\
{\partial L_{k,l} \over \partial w_{i,j}} &= {\partial L_{k,l} \over \partial w_{i,j}} = {w_{k,l} \over 2\sqrt{d_k}d_l^{3\over2}}
\end{aligned}
\end{equation}

\end{document}
